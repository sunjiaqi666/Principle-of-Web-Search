{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import jieba\n",
    "\n",
    "class SearcherIIndex():\n",
    "    \"\"\"倒排索引文本搜索实现类\n",
    "    \n",
    "    用倒排索引\n",
    "    利用Python的集合运算，来实现候选结果集之间交、并运算\n",
    "    \n",
    "    Attributes:\n",
    "        index: 检索使用的倒排索引\n",
    "        max_id: 当前索引的文档最大ID\n",
    "        doc_list: 索引文档原文8\n",
    "    \"\"\"\n",
    "    def __init__(self, docs_file): \n",
    "        \"\"\"初始化，用文件中的文本行构建倒排索引\n",
    "        \n",
    "        Args:\n",
    "            docs_file:包含带索引文档(文本)的文件名\n",
    "            \n",
    "        \"\"\"\n",
    "        self.index = dict()    \n",
    "        self.max_id = 0\n",
    "        self.doc_list = [] \n",
    "        \n",
    "        with open(docs_file, 'r') as f:\n",
    "            docs_data = f.read()\n",
    "        \n",
    "        for doc in docs_data.split():\n",
    "            self.add_doc(doc)\n",
    "\n",
    "    def add_doc(self, doc):\n",
    "        \"\"\"向索引中添加新文档\n",
    "        \n",
    "        Args:\n",
    "            doc:待检索的文档(文本)\n",
    "        \n",
    "        Returns:\n",
    "            新增文档ID\n",
    "        \"\"\"\n",
    "        self.doc_list.append(doc)\n",
    "        for term in list(jieba.cut_for_search(doc)):\n",
    "            #构建和更新各Term对应的Posting(集合)\n",
    "            if term in self.index: \n",
    "                self.index[term].add(self.max_id)\n",
    "            else:\n",
    "                self.index[term] = set([self.max_id])\n",
    "        self.max_id += 1\n",
    "        return self.max_id - 1\n",
    "    \n",
    "    def word_match(self, word):\n",
    "        \"\"\"从倒排索引中获取包含word的候选文档ID集合\n",
    "        \n",
    "        Args:\n",
    "            word:待检索的词(短语)\n",
    "            \n",
    "        Returns：\n",
    "            包含待检索词(短语)的文档ID集合\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        for term in list(jieba.cut(word)):\n",
    "            if result is None:\n",
    "                result = self.index.get(term, set())\n",
    "            else:\n",
    "                result = result & self.index.get(term, set())\n",
    "        if result is None:\n",
    "            result = set()\n",
    "        return result\n",
    "\n",
    "    def conv_query(self, query):\n",
    "        \"\"\"将用户的查询转换成用eval可运行、返回结果ID集合的代码段\n",
    "        \n",
    "        Args:\n",
    "            query:待转换的原始查询字符串\n",
    "        \n",
    "        Returns:\n",
    "            转换完成可通过eval执行返回ID集合的代码段字符串\n",
    "        \"\"\"\n",
    "        query_new_parts = []\n",
    "        all_parts = list(jieba.cut(query))\n",
    "        idx = 0\n",
    "        cache = '' #缓存变量，用于回收分词过程被切开的短语片段\n",
    "        count_parts = len(all_parts)\n",
    "        while idx < count_parts:\n",
    "            if all_parts[idx] == '(' or all_parts[idx] == ')':\n",
    "                query_new_parts.append(all_parts[idx])\n",
    "            elif all_parts[idx] == ' ':\n",
    "                query_new_parts.append(' ')\n",
    "            elif all_parts[idx] in ('and', 'AND', '+'):\n",
    "                query_new_parts.append('&')\n",
    "            elif all_parts[idx] in ('or', 'OR'):\n",
    "                query_new_parts.append('|')\n",
    "            elif all_parts[idx] in ('not', 'NOT', '-'):\n",
    "                query_new_parts.append('-')\n",
    "            elif (idx + 1 < count_parts #被分词切开的短语部分回收至缓存\n",
    "                  and all_parts[idx+1] not in (' ', ')')): \n",
    "                cache += all_parts[idx]\n",
    "            elif (idx + 2 < count_parts #处理词间空格的形式\n",
    "                  and all_parts[idx+1] == \" \" \n",
    "                  and all_parts[idx+2] not in ('(', ')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ')): \n",
    "                query_new_parts.append(\"self.word_match('{}') & \".format(all_parts[idx]))\n",
    "                idx += 2\n",
    "                continue\n",
    "            else:\n",
    "                query_new_parts.append(\"self.word_match('{}')\".format(cache + all_parts[idx]))\n",
    "                cache = '' #合并完成清空缓存\n",
    "            idx += 1\n",
    "        query_new = ''.join(query_new_parts)\n",
    "        return query_new\n",
    "\n",
    "    def highlighter(self, doc, word):\n",
    "        \"\"\"用word对doc进行HTML高亮\n",
    "        \n",
    "        Args:\n",
    "            doc:需要高亮的文档\n",
    "            word:要进行高亮的关键词(查询)\n",
    "            \n",
    "        Returns:\n",
    "            返回对关键词(查询)进行高亮的文档\n",
    "        \"\"\"\n",
    "        for part in list(jieba.cut(word)):\n",
    "            #TODO(CHG):短语高亮需要先分词\n",
    "            if part not in ('(', ')', 'and', 'AND', 'or', 'OR', 'NOT', 'not', ' '):\n",
    "                doc = doc.replace(part, '<span style=\"color:red\">{}</span>'.format(part))\n",
    "        return doc\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"用query进行查询返回结果文档列表\n",
    "        \n",
    "        Args:\n",
    "            query:用户的(复合)布尔查询字符串\n",
    "            \n",
    "        Returns:\n",
    "            复合查询要求的(高亮)文档结果列表\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        query_new = self.conv_query(query)\n",
    "        for did in eval(query_new):\n",
    "            result.append(self.highlighter(self.doc_list[did], query))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class SearcherIIndexVII(SearcherIIndex):\n",
    "    \"\"\"倒排索引文本搜索实现类(改进)\n",
    "    \n",
    "    自定义解析，保留英文片段，将中文片段多粒度分词处理\n",
    "    \n",
    "    Attributes:\n",
    "        index: 检索使用的倒排索引\n",
    "        max_id: 当前索引的文档最大ID\n",
    "        doc_list: 索引文档原文\n",
    "    \"\"\"\n",
    "    def parse_doc(self, doc):\n",
    "        \"\"\"对文档进行自定义解析，保留英文串，对中文串多粒度分词\n",
    "        \n",
    "        Args:\n",
    "            doc:待解析的原始文档\n",
    "        \n",
    "        Returns:\n",
    "            解析结果列表，元素是切分得到的term\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        state_last = ''\n",
    "        cache = ''\n",
    "        for c in doc:\n",
    "            state_c = c in string.ascii_letters \\\n",
    "                or c.isdigit() \\\n",
    "                or c in ('-', ':', '.')\n",
    "            if c == ' ':\n",
    "                if state_last:\n",
    "                    result.append(cache)\n",
    "                else:\n",
    "                    result.extend(list(jieba.cut_for_search(cache)))\n",
    "                result.append(' ')\n",
    "                cache = ''\n",
    "                state_last = '' \n",
    "            else:\n",
    "                if state_c == state_last:\n",
    "                    cache += c\n",
    "                else:\n",
    "                    if state_last != '':\n",
    "                        if state_last:\n",
    "                            result.append(cache)\n",
    "                        else:\n",
    "                            result.extend(list(jieba.cut_for_search(cache)))\n",
    "                    cache = c\n",
    "                state_last = state_c\n",
    "        if cache:\n",
    "            if state_last:\n",
    "                result.append(cache)\n",
    "            else:\n",
    "                result.extend(list(jieba.cut_for_search(cache)))\n",
    "        return result\n",
    "    \n",
    "    def add_doc(self, doc):\n",
    "        \"\"\"向索引中添加新文档(正常索引及ngram索引)\n",
    "        \n",
    "        Args:\n",
    "            doc:待检索的文档(文本)\n",
    "        \n",
    "        Returns:\n",
    "            新增文档ID\n",
    "        \"\"\"\n",
    "        self.doc_list.append(doc)\n",
    "        doc = doc.lower()\n",
    "        for term in self.parse_doc(doc):\n",
    "            #构建和更新各Term对应的Posting(集合)\n",
    "            if term in self.index: \n",
    "                self.index[term].add(self.max_id)\n",
    "            else:\n",
    "                self.index[term] = set([self.max_id])\n",
    "        \n",
    "        #构建ngram索引(以二元为例)\n",
    "        doclen = len(doc)\n",
    "        for i in range(doclen-1):\n",
    "            term = doc[i:i+2]\n",
    "            if term in self.index_b: \n",
    "                self.index_b[term].add(self.max_id)\n",
    "            else:\n",
    "                self.index_b[term] = set([self.max_id])\n",
    "                \n",
    "        self.max_id += 1\n",
    "        return self.max_id - 1\n",
    "    \n",
    "    def dumpIndex(self):\n",
    "        \"\"\"原样输出索引，用于检查索引构建结果\n",
    "        \n",
    "        Returns:\n",
    "            对索引(字典结构)的Dump输出\n",
    "        \"\"\"\n",
    "        print(self.index)\n",
    "    \n",
    "    def conv_query(self, query):\n",
    "        \"\"\"将用户的查询转换成用eval可运行、返回结果ID集合的代码段\n",
    "        \n",
    "        Args:\n",
    "            query:待转换的原始查询字符串\n",
    "        \n",
    "        Returns:\n",
    "            转换完成可通过eval执行返回ID集合的代码段字符串\n",
    "        \"\"\"\n",
    "        query_new_parts = []\n",
    "        all_parts = list(self.parse_query(query))\n",
    "        idx = 0\n",
    "        cache = '' #缓存变量，用于回收分词过程被切开的短语片段\n",
    "        count_parts = len(all_parts)\n",
    "        while idx < count_parts:\n",
    "            if all_parts[idx][1:] == '(' or all_parts[idx][1:] == ')':\n",
    "                query_new_parts.append(all_parts[idx][1:])\n",
    "            elif all_parts[idx][1:] == ' ':\n",
    "                query_new_parts.append(' ')\n",
    "            elif all_parts[idx][1:] in ('and', 'AND', '+'):\n",
    "                query_new_parts.append('&')\n",
    "            elif all_parts[idx][1:] in ('or', 'OR'):\n",
    "                query_new_parts.append('|')\n",
    "            elif all_parts[idx][1:] in ('not', 'NOT', '-'):\n",
    "                query_new_parts.append('-')\n",
    "            elif (idx + 1 < count_parts #对连续的内容分段结果集合中间加”&“运算符\n",
    "                  and all_parts[idx+1][1:] not in (' ', ')')): \n",
    "                query_new_parts.append(\"{} & \".format(self.conv_part(all_parts[idx])))\n",
    "            elif (idx + 2 < count_parts #处理词间、词与符号间空格的情况\n",
    "                  and all_parts[idx+1][1:] == \" \" \n",
    "                  and all_parts[idx+2][1:] not in (')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ')): \n",
    "                query_new_parts.append(\"{} & \".format(self.conv_part(all_parts[idx])))\n",
    "                idx += 2\n",
    "                continue\n",
    "            else:\n",
    "                query_new_parts.append(self.conv_part(cache + all_parts[idx]))\n",
    "                cache = '' #合并完成清空缓存\n",
    "            idx += 1\n",
    "        query_new = ''.join(query_new_parts)\n",
    "        return query_new\n",
    "    \n",
    "    def term_match(self, term):\n",
    "        \"\"\"在索引里找到term对应的posting集合\n",
    "        \n",
    "        Args:\n",
    "            term:要检索的词项\n",
    "            \n",
    "        Results:\n",
    "            term对应的posting集合\n",
    "        \"\"\"\n",
    "        return self.index.get(term, set()) \n",
    "    \n",
    "    def conv_part(self, part):\n",
    "        \"\"\"将带有类别标记的解析结果段 转化为 eval能进行计算的代码段\n",
    "        \n",
    "        Args:\n",
    "            part:带有类别标记的解析结果段\n",
    "            \n",
    "        Results:\n",
    "            eval能进行计算的代码段字符串(调用 term_match() 进行计算)\n",
    "        \"\"\"\n",
    "        flag = part[0]\n",
    "        if flag == 'e':\n",
    "            return \"self.term_match('{}')\".format(part[1:])\n",
    "        elif flag == 'c':\n",
    "            return \"(self.term_match('{}'))\".format(\n",
    "                \"') & self.term_match('\".join(jieba.cut(part[1:])))\n",
    "        elif flag == 'f':\n",
    "            return \"self.frag_match('{}')\".format(part[1:])\n",
    "    \n",
    "    def highlighter(self, doc, query):\n",
    "        \"\"\"用query对doc进行HTML高亮\n",
    "        \n",
    "        Args:\n",
    "            doc:需要高亮的文档\n",
    "            word:要进行高亮的关键词(查询)\n",
    "            \n",
    "        Returns:\n",
    "            返回对关键词(查询)进行高亮的文档\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        #生成要进行高亮的关键词串集合\n",
    "        word_set = set()\n",
    "        query = query.lower()\n",
    "        query_parts = self.parse_query(query)\n",
    "        for query_part in query_parts:\n",
    "            if query_part[0] == 'e' or query_part[0] == 'f':\n",
    "                word_set.add(query_part[1:])\n",
    "                if len(query_part[1:]) > n:\n",
    "                    n = len(query_part[1:])\n",
    "            elif query_part[0] == 'c':\n",
    "                if len(query_part[1:]) > 1:\n",
    "                    for word in jieba.cut(query_part[1:]):\n",
    "                        word_set.add(word)\n",
    "                        if len(word) > n:\n",
    "                            n = len(word)\n",
    "        \n",
    "        #遍历文档替换高亮关键词串\n",
    "        doc_low = doc.lower()\n",
    "        i = 0\n",
    "        result = []\n",
    "        while True:\n",
    "            end_idx = i + n\n",
    "            if end_idx > len(doc_low):\n",
    "                end_idx = len(doc_low)\n",
    "            for j in range(end_idx, i, -1):\n",
    "                if doc_low[i:j] in word_set:\n",
    "                    break\n",
    "            if doc_low[i:j] in word_set:\n",
    "                result.append(\n",
    "                    '<span style=\"color:red\">{}</span>'.format(doc[i:j]))\n",
    "            else:\n",
    "                result.append(doc_low[i:j])\n",
    "            i = j\n",
    "            if i == len(doc_low):\n",
    "                break\n",
    "                \n",
    "        return ''.join(result)\n",
    "    \n",
    "    def __init__(self, docs_file): \n",
    "        \"\"\"初始化，用文件中的文本行构建倒排索引\n",
    "        \n",
    "        Args:\n",
    "            docs_file:包含带索引文档(文本)的文件名\n",
    "            \n",
    "        \"\"\"\n",
    "        self.index = dict() #标准倒排索引\n",
    "        self.index_b = dict() #ngram索引\n",
    "        self.max_id = 0\n",
    "        self.doc_list = [] \n",
    "        \n",
    "        with open(docs_file, 'r') as f:\n",
    "            docs_data = f.read()\n",
    "        \n",
    "        for doc in docs_data.split():\n",
    "            self.add_doc(doc)\n",
    "            \n",
    "    def frag_match(self, frag):\n",
    "        \"\"\"对片段frag用ngram索引实现原样搜索\n",
    "        \n",
    "        Args:\n",
    "            frag:要原样搜索的字符串\n",
    "            \n",
    "        Results:\n",
    "            片段原样搜索的结果(文档ID)集合\n",
    "        \"\"\"\n",
    "        frag = frag.lower() #大小写归一化\n",
    "        result = None\n",
    "        doclen = len(frag)\n",
    "        for i in range(doclen - 1):\n",
    "            term = frag[i:i+2]\n",
    "            if result is None:\n",
    "                result = self.index_b.get(term, set())\n",
    "            else:\n",
    "                result = result & self.index_b.get(term, set())\n",
    "        return result\n",
    "    \n",
    "    def get_char_type(self, c):\n",
    "        \"\"\"返回当前字符的类型(e,c,s,f,b)\n",
    "        \n",
    "        Args:\n",
    "            c:要进行判断的单个字符\n",
    "            \n",
    "        Results:\n",
    "            返回判断结果(前缀)：e为英文，c为中文，s为空格，f为引号，b为括号\n",
    "        \"\"\"\n",
    "        result = 'c'\n",
    "        if c in string.ascii_letters \\\n",
    "                or c.isdigit() \\\n",
    "                or c in ('-', ':', '.'):\n",
    "            result = 'e'\n",
    "        elif c == '\"':\n",
    "            result = 'f'\n",
    "        elif c == ' ':\n",
    "            result = 's'\n",
    "        elif c in ('(', ')'):\n",
    "            result = 'b'\n",
    "        return result\n",
    "    \n",
    "    def parse_query(self, doc):\n",
    "        \"\"\"对查询进行自定义解析，保留英文串，对中文串原型插入\n",
    "        \n",
    "        Args:\n",
    "            doc:待解析的原始文档\n",
    "        \n",
    "        Returns:\n",
    "            解析结果列表，元素是带有串类型标记(首字符，e为英文，c为中文，s为空格，f为引号，b为括号)\n",
    "            的切分term结果\n",
    "        \"\"\"\n",
    "        doc = doc.lower() + ' ' #解决末位字符状态切换问题的小技巧\n",
    "        result = []\n",
    "        doclen = len(doc)\n",
    "        i = 0\n",
    "        while True:\n",
    "            cur_char_type = self.get_char_type(doc[i])\n",
    "            for j in range(i+1, doclen):\n",
    "                if cur_char_type == 'f': #当前符号为引号，找下一个引号\n",
    "                    if self.get_char_type(doc[j]) == 'f':\n",
    "                        break\n",
    "                elif self.get_char_type(doc[j]) != cur_char_type: #当前符号非引号，找下一个状态变化\n",
    "                    break\n",
    "            if cur_char_type == 's': #对多个空格连续出现的情况进行合并\n",
    "                result.append('s ')\n",
    "            elif cur_char_type == 'f': #对引号只提取引号内字符串\n",
    "                result.append(cur_char_type + doc[i+1:j])\n",
    "                j += 1\n",
    "            else:\n",
    "                result.append(cur_char_type + doc[i:j])\n",
    "            i = j\n",
    "            if i >= doclen - 1:\n",
    "                break\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f七连胜']\n",
      "['一边倒！中国女排3-0横扫美国取<span style=\"color:red\">七连胜</span>', '一边倒！中国女排3-0横扫美国取<span style=\"color:red\">七连胜</span>', '一边倒！中国女排3-0横扫美国取<span style=\"color:red\">七连胜</span>', '一边倒！中国女排3-0横扫美国取<span style=\"color:red\">七连胜</span>', '一边倒！中国女排3-0横扫美国取<span style=\"color:red\">七连胜</span>']\n"
     ]
    }
   ],
   "source": [
    "query = '\"七连胜\"'\n",
    "searcher = SearcherIIndexVII('titles.txt')\n",
    "print(searcher.parse_query(query))\n",
    "print(searcher.search(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fmate']\n",
      "self.frag_match('mate')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "外媒评<span style=\"color:red\">Mate</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:red\">Mate</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "怼完苹果怼三星，刚发了<span style=\"color:red\">Mate</span>30的余承东依然要"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "华为<span style=\"color:red\">Mate</span>30采用安卓系统"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "可安装谷歌gms！华为<span style=\"color:red\">Mate</span>30确认支持boo"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:red\">Mate</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "余承东:<span style=\"color:red\">Mate</span>30"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "iphone11和华为<span style=\"color:red\">Mate</span>30拍照对比：差距"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:red\">Mate</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "外媒评<span style=\"color:red\">Mate</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "怼完苹果怼三星，刚发了<span style=\"color:red\">Mate</span>30的余承东依然要"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "外媒评华为<span style=\"color:red\">Mate</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "余承东:<span style=\"color:red\">Mate</span>30"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "华为<span style=\"color:red\">Mate</span>30采用安卓系统"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "华为发布<span style=\"color:red\">Mate</span>30系列手机：电池最大4500m"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "怼完苹果怼三星，刚发了<span style=\"color:red\">Mate</span>30的余承东依然要"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "searcher = SearcherIIndexVII('titles.txt')\n",
    "\n",
    "query = '\"mate\"'\n",
    "print(searcher.parse_query(query))\n",
    "print(searcher.conv_query(query))\n",
    "result = searcher.search(query)\n",
    "if result:\n",
    "    for doc in result:\n",
    "        display(HTML(doc))\n",
    "else:\n",
    "    print('No result.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.words.words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "class Corrector():\n",
    "    \"\"\"用二元索引实现拼写校正\n",
    "    \n",
    "    Attributes:\n",
    "        index_b: 检索使用的二元索引\n",
    "        max_id: 当前索引的单词最大ID\n",
    "        doc_list: 索引单词原文\n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        \"\"\"初始化，用NLTK的words词典构建倒排索引\n",
    "        \"\"\"\n",
    "        self.index_b = dict() #ngram索引\n",
    "        self.max_id = 0\n",
    "        self.doc_list = [] \n",
    "        \n",
    "        for doc in nltk.corpus.words.words():\n",
    "            self.add_doc(doc)\n",
    "            \n",
    "    def add_doc(self, doc):\n",
    "        \"\"\"向索引中添加新词(单词的二元索引)\n",
    "        \n",
    "        Args:\n",
    "            doc:待检索的单词\n",
    "        \n",
    "        Returns:\n",
    "            新增单词ID\n",
    "        \"\"\"\n",
    "        self.doc_list.append(doc)\n",
    "        doc = doc.lower()\n",
    "        \n",
    "        #构建二元索引\n",
    "        doclen = len(doc)\n",
    "        for i in range(doclen-1):\n",
    "            term = doc[i:i+2]\n",
    "            if term in self.index_b: \n",
    "                self.index_b[term].append(self.max_id)\n",
    "            else:\n",
    "                self.index_b[term] = [self.max_id]\n",
    "                \n",
    "        self.max_id += 1\n",
    "        return self.max_id - 1\n",
    "    \n",
    "    def correct(self, word, limit=5):\n",
    "        \"\"\"拼写校正函数\n",
    "        \n",
    "        Args:\n",
    "            word:待校正的词\n",
    "            limit:返回结果的最大条数，默认值为5\n",
    "            \n",
    "        Returns:\n",
    "            最可能的校正单词列表\n",
    "        \"\"\"\n",
    "        word = word.lower() #大小写归一化\n",
    "        result = []\n",
    "        docid_list = []\n",
    "        doclen = len(word)\n",
    "        for i in range(doclen - 1):\n",
    "            term = word[i:i+2]\n",
    "            docid_list += self.index_b.get(term, [])\n",
    "        docid_counter = Counter(docid_list)\n",
    "        count = 0\n",
    "        for elem in docid_counter.most_common(300):\n",
    "            cor_word = self.doc_list[elem[0]]\n",
    "            if len(cor_word) >= doclen - 1 and len(cor_word) <= doclen + 1:\n",
    "                result.append(cor_word)\n",
    "                count += 1\n",
    "                if count > limit:\n",
    "                    break\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = Corrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retrieval', 'pretribal', 'rearrival', 'rerival', 'retreatal', 'retrial']\n"
     ]
    }
   ],
   "source": [
    "print(cor.correct('retrival'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 1, 3: 1, 4: 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([1,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-13-c918f0d68a99>\u001b[0m(168)\u001b[0;36mconv_query\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    166 \u001b[0;31m                \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;31m#合并完成清空缓存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    167 \u001b[0;31m            \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 168 \u001b[0;31m        \u001b[0mquery_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_new_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    169 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mquery_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    170 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  quit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "华为|M|a|t|e|3|0|采|用|安卓系统\n",
      "华为|M|a|t|e|3|0|采|用|安卓系统\n"
     ]
    }
   ],
   "source": [
    "doc = '华为Mate30采用安卓系统'\n",
    "\n",
    "n = 6\n",
    "word_set = set(\n",
    "    ['华为' ,'安卓', '安卓系统'])\n",
    "\n",
    "#正向最大分词\n",
    "i = 0\n",
    "result_f = []\n",
    "while True:\n",
    "    end_idx = i + n\n",
    "    if end_idx > len(doc):\n",
    "        end_idx = len(doc)\n",
    "    for j in range(end_idx, i, -1):\n",
    "        if doc[i:j] in word_set:\n",
    "            break\n",
    "    result_f.append(doc[i:j])\n",
    "    i = j\n",
    "    if i == len(doc):\n",
    "        break\n",
    "print('|'.join(result_f))\n",
    "\n",
    "#逆向最大分词\n",
    "i = len(doc)\n",
    "result_b = []\n",
    "while True:\n",
    "    end_idx = i - n\n",
    "    if end_idx < 0:\n",
    "        end_idx = 0\n",
    "    for j in range(end_idx, i):\n",
    "#         print(j,i,doc[j:i])\n",
    "        if doc[j:i] in word_set:\n",
    "            break\n",
    "    result_b.insert(0, doc[j:i])\n",
    "    i = j\n",
    "    if i == 0:\n",
    "        break\n",
    "print('|'.join(result_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#列表倒序的简单方法：切片法\n",
    "[1, 2, 3, 4, 5][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 采集网易新闻排行新闻标题、URL列表 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "news_list = []\n",
    "url = 'http://news.163.com/rank/'\n",
    "r = requests.get(url) #下载\n",
    "sel = etree.HTML(r.text) #解析\n",
    "nodes = sel.xpath('//td/a')\n",
    "for node in nodes:\n",
    "    news_list.append([node.text, node.attrib['href']])\n",
    "    \n",
    "with open('163news.csv','w') as f: #入库\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerows(news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "南充小伙老家建洋气别墅 一栋自己住一栋送堂哥,https://home.163.com/19/1209/07/EVUGCN4I001081EI.html\n",
      "恋情实锤？杨幂魏大勋入住同一酒店没出来,https://ent.163.com/19/1210/08/F017MIA700038FO9.html\n",
      "奥巴马花8260万买豪宅 11.7万㎡大庄园拥私,https://home.163.com/19/1210/07/F012SVAN001081EI.html\n",
      "39岁董洁扮嫩演高中生 被吐槽比剧中姑妈还老气,https://ent.163.com/19/1209/07/EVUJ6ND000038FO9.html\n",
      "山西古村被“圈”收门票 九成都是危房只剩老人居住,https://home.163.com/19/1209/07/EVUGCSBR001081EI.html\n",
      "\"\"\"冰花男孩\"\"父亲申请贫困户遭拒 村主任:其名下有\",https://news.163.com/19/1209/09/EVUP0P590001899O.html\n",
      "五星级酒店竟是“毛坯房”?一晚9万6人们排队住,https://home.163.com/19/1209/07/EVUGCTIM001081EI.html\n",
      "心虚?李小璐晒旅游照又秒删 疑与PG One国外,https://ent.163.com/19/1209/09/EVUPOMRO00038FO9.html\n",
      "62岁老人自制创意鲁班家具 不用一颗钉子还可折叠,https://home.163.com/19/1210/07/F012SP99001081EI.html\n",
      "飞机起飞前乘客收到亲人噩耗 航班紧急滑回航站楼,https://news.163.com/19/1209/11/EVV0NUPF0001875P.html\n"
     ]
    }
   ],
   "source": [
    "!head 163news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### 采集新闻正文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://play.163.com/photoview/ITPG0031/95896.html\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "from tomorrow import threads\n",
    "\n",
    "#用tomorrow实现多线程采集\n",
    "@threads(5)\n",
    "def download(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "news_list = []\n",
    "with open('163news.csv', 'r') as f:\n",
    "    f_csv = csv.reader(f)\n",
    "    for row in f_csv:\n",
    "        news_list.append([row[0], row[1]])\n",
    "        \n",
    "result_detail = []\n",
    "\n",
    "responses = [download(item[1]) for item in news_list[566:]]\n",
    "for r in responses:\n",
    "    url = r.url\n",
    "    sel = etree.HTML(r.text)\n",
    "    try:\n",
    "        title = sel.xpath(\"//title/text()\")[0]\n",
    "        if 'dy.163.com' in url:\n",
    "            nodes = sel.xpath(\"//div[@id='content']/p/text()\")\n",
    "        else:\n",
    "            nodes = sel.xpath(\"//div[@id='endText']/p/text()\")\n",
    "        content = ''.join(map(lambda x: x.strip(), nodes))\n",
    "        if(len(content) == 0):\n",
    "            print(url)\n",
    "            break\n",
    "    except Exception:\n",
    "        print(url)\n",
    "        break\n",
    "    result_detail.append([url, title, content])\n",
    "#     print(url, len(content))\n",
    "    \n",
    "with open('163news_d.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerows(result_detail)\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://home.163.com/19/1209/07/EVUGCN4I001081EI.html,南充小伙老家建洋气别墅 一栋自己住一栋送堂哥_网易家居,四川是一个山清水秀的好地方，本着地理优势，成为国内的旅游大省。近些年农村兴起的建房热，作为四川人，可当然不能浪费了家乡优美的自然环境。南充一小伙就投身成为了建房热中的一员。在自己老家的山脚下建起了别墅，还不是一栋，建了两栋，一栋自己住，一栋送给堂哥家。有钱就是任性啊，快跟家居君来看看。这是小伙家之前的老宅，传统的瓦房，已经有着不少的岁月痕迹了。拆除了旧房子，平整场地之后直接开挖，打地基。小伙家建的是两层别墅，一层结构完成之后，工人师傅们很快就开始了二层的基础工作。因为不是北方，有些农业大省，需要平顶房在房顶晾晒粮食。所以采用了美观的斜坡式封顶，也更利于雨天的排水。蓝色的瓦片一层叠一层很是好看。房子终于完工了，可以看到整个的面积很大很气派，整整有191㎡。浅黄色的外墙和蓝顶的双色搭配，洋气！小伙堂哥家的经济状况不是很好，小伙另外建了一栋一层的小别墅送给堂哥家。完工得很快。配色风格和自家的是一样的，外墙贴上黄色瓷砖，房顶是蓝色瓦片。但是面积小了一些，只有104㎡，不过也足够一家人正常生活了。【】这位四川小伙真是仗义，知道堂哥家经济状况不好，另建了一栋小别墅送给堂哥，这钱花的有排面。虽说这两栋别墅价值不菲，但是现在农村建房的人，不缺钱的可真不少。比如下面这湖南农村一家，承包建设的别墅可是100多万，室内还有大KTV房，（来源：田园雅墅，由网易家居综合整理）\n",
      "https://ent.163.com/19/1210/08/F017MIA700038FO9.html,恋情实锤？杨幂魏大勋入住同一酒店没出来_网易娱乐,12月3日，魏大勋被拍到与杨幂前后脚进同一家酒店，疑似恋情再添实锤。爆料称，魏大勋进入杨幂剧组酒店后就没再出现，一直到第二天早上六点左右，他才全副武装偷偷溜出酒店，走出酒店大门后还恋恋不舍地回头朝酒店里张望，随后便大步流星离开了酒店。据悉，此前就有媒体曝光杨幂与魏大勋住所中间只隔一条马路，七月时还有媒体曝光魏大勋曾前往大连剧组探班杨幂，八月被网友拍到两人穿同款鞋共游798艺术区，前几日又被网友扒出戴了使用痕迹一模一样的帽子。此前，杨幂和魏大勋多次被传恋爱绯闻，两人曾被网友遇到一同亲密逛街，虽然魏大勋发文表示只是朋友，但随后两人更被扒出多个同款衣物，就连帽子都因划痕一致被指戴的是同一顶。魏大勋和杨幂曾一同拍摄综艺节目。\n",
      "https://home.163.com/19/1210/07/F012SVAN001081EI.html,奥巴马花8260万买豪宅 11.7万㎡大庄园拥私人海滩_网易家居,\"离奥巴马卸任美国总统已经过去了将近三年的时间，自从他离任后，就鲜少出现在大众视野中。当现总统川普因为国事焦头烂额的时候，奥巴马那边却过上了舒心惬意的养老生活。不久前，他和妻子大手一挥，花1,175万美元的价格买下了一套豪华的海滨度假房产。折算成人民币差不多是8260万元，四舍五入就是一个亿啊。不是，原来当总统这么赚钱的吗？这套房产占地11.7公顷，还有私人海滩和船屋，来跟家居君一起看看吧。这套房产位于马萨诸塞州，前总统肯尼迪和夫人也曾在这里买房居住。毕竟是南部沿海地区，气候温暖，环境优美，本身就是度假胜地，当然是很受富豪们喜欢的。奥巴马的这套房产始建于2001年，整个庄园内绿化非常好，不是草坪就是树木，大道从豪宅门口直接通到海边。庄园内的大宅子占地6900平方英尺，也就是641平方米，相当的大。两层豪宅包含7间卧室9个卫生间，宅前的泳池旁摆满了躺床，可随意享受日光浴。房间整体的装修风格是白色的，清新明亮的白色占了绝大部分视觉空间，连家具的选择都是一体的白。客厅里是白沙发白色落地灯，只有一些小件家具的不同色彩起到点缀作用。卧室同样是如此，除了红木地板和床头两张实木桌之外，目之所及都是明亮的白。家居君属实是有点接受无能，这样的房间呆久了不会觉得视觉疲劳，累吗？二楼的外面设置了几个露天阳台，这三张躺床一摆，也是极佳的日光浴位置，庄园内的风光尽收眼底，前面的草坪还可以用来停放私人直升机。【】白宫作为总统居住和办公的地方，一直都是很神秘的存在。看了奥巴马的豪宅庄园，来瞅瞅他的白宫居室吧，不得不说，和川普比起来简直太有品位。（来源：英伦房产圈，由网易家居综合整理）\"\n",
      "https://ent.163.com/19/1209/07/EVUJ6ND000038FO9.html,39岁董洁扮嫩演高中生 被吐槽比剧中姑妈还老气_网易娱乐,\"近日,由任达华和董洁主演的新剧正在热播中，该剧以回归二十年的澳门日常做背景，讲述了二十年来澳门普通人家最朴素逼真的伦常生活,任达华和董洁在剧中扮演一对父女,因为时间跨度关系,董洁还扮演了少女期的女主,因此引发了不少争议。刚开始,剧中梁舒这个角色还是小少女的模样,一转眼就成了董洁未免让人感到出戏,并且扮演的还是18岁的高三学生。董洁出场时一身学院风的衣服，梳着很少女的发型。光看这造型，还是挺青春的,仔细一看却疲态尽显,怎么都不像高中生。无论美颜滤镜有多强都很难像少女。多厚的柔光美颜滤镜，依然挡不住董洁脸上那深深的法令纹。就连剧中的姑妈扮演着看着都要比董洁年轻,不看造型根本分不清谁是谁姑妈。\"\n",
      "https://home.163.com/19/1209/07/EVUGCSBR001081EI.html,山西古村被“圈”收门票 九成都是危房只剩老人居住_网易家居,这几年古村落的发展越来越受到重视了，很多乡村旅游项目也大受欢迎。文化能被保护，我们又能收获快乐，这原本是两全其美的事情，可是最近家居姐才发现事实并非如此。山西的大汖村就是一个著名的旅游景点，然而全村竟然有九成的房子都面临着坍塌的危险，这样的景区还能这么受欢迎？真不知道该高兴还是该难过。这个山村因为地貌奇特，早在2013年的时候就被列为国家级的传统村落，人们更是称其为“深山里的布达拉宫”。这样听起来，是不是很想去这个地方看看呢，但是进到了村庄里面，你的心情可能就不会这么好了~整个村庄有将近九成的危房，随时都可能会倒塌。这是个什么概念呢，可能你正兴致勃勃地参观拍照，就会面临随时被砸的危险。村民们的家都是由一张炕，几个简单的家具组成的，吃穿住都在一个房间，过得非常简陋。窗户和门感觉从来没有换过，不知道大风吹来的时候能不能抗住。可能好点的家庭还有一台电视机，每天看看电视也是不错的，但要是碰上断电就什么也干不成了。他们日常的活动可能就是互相串门唠嗑，完全没有什么娱乐项目。因为环境条件太差，这里现在也只剩十几户人家了，大部分还是独居的老人。原本开发项目对他们而言是一件好事，但是开发商只是用钱租借了几片区域，并不想去修复这些建筑，村民们心里还是有点失落的。将来这里的居民只会越来越少，我们所看到的美好也在逐渐消失~很多开放商觉得修复农村很困难，其实模拟再造也是一种保护。朝鲜有个村庄就是由国家建盖的，沿用传统村落的房屋构造。政府还分配了农民进行民宿事业的经营~以前只听说过推进城市化进程，没想到朝鲜还建起了小农村。这个农村景点与大汖村有着天壤之别，村民们不仅业余生活丰富，居住环境更是舒适。屋内暗藏地窖式厨房，不仅能做饭还可以取暖，一家人的生活充满幸福感。（来源：财经网，由网易家居综合整理）\n",
      "https://news.163.com/19/1209/09/EVUP0P590001899O.html,\"\"\"冰花男孩\"\"父亲申请贫困户遭拒 村主任:其名下有车_网易新闻\",\"（原标题：冰花男孩父亲申请贫困户遭拒  村主任：其名下有车不合标准）【#冰花男孩父亲申请贫困户遭拒# \n",
      "村主任：其名下有车不合标准】12月7日，“冰花男孩”父亲王刚奎发文称自己申请贫困户、妻子和母亲申请村里的扫地工作遭拒。该村村主任王刚明表示，王刚奎名下拥有两辆机动车，不符合贫困户标准，不是贫困户则不能申请扫地等公益性岗位。\"\n",
      "https://home.163.com/19/1209/07/EVUGCTIM001081EI.html,五星级酒店竟是“毛坯房”?一晚9万6人们排队住_网易家居,在纽约曼哈顿这样的城市CBD，富丽堂皇的奢华酒店是从来不缺的，它们往往内饰华丽，装修中透露出金钱的气息。但是在这些酒店中，有一家，却是一股与众不同的清流。其中最尊贵的套房达到了每晚1万5美金的价格，折合人民币就是9万6，这么贵不是应该见怪不怪了吗？然而这个套房里面是这样的：对，你没看错，没有粉刷的墙壁和地面，房间墙体都是灰白的水泥，这不就是毛坯房吗？还真有人花9万6去住？别说，就这样的毛坯房，却引得富豪们排队疯抢。想知道为什么？那就跟着家居君来看看吧。其实这套名为翠贝卡的阁楼套房出自比利时的天才艺术家之手，同时也是一名室内设计师兼收藏家，他始终信奉着简单、纯粹的设计美学。年幼时，维伍德就跟着母亲进行过古老街区的保护工作。上世纪70年代初，维伍德到过泰国、柬埔寨和日本后，被佛教艺术的恬静，寺庙建筑的祥和深深吸引了。从此形成了他安静、朴素的空间艺术风格。这套公寓整体围绕着水泥墙体的灰白色调，选用了纯木色的家具，灰白色的沙发，一些同色调的原汁原味的艺术摆件。除此之外没有其他过多装饰，现代化的设备一概没有，比起现在流行的简约风不知简单到哪里去了。锈红色的木门，充满年代感的木质家具和木质吊顶。灰白色的墙壁和床铺，房间内放置了一个石头摆件。你说这是寺庙里的和尚的卧室我也信。浴室里依旧是原石加原木，不过墙壁和地面都是采用深色的石头建造，有一种幽静神秘之感。浴池就是一个石槽，旁边放置了一个小木凳，墙内的壁炉能在洗澡时保持室内温度。右边有一页深蓝色木门，这个颜色有一种现代的高级感，但是在整个房间内却并不显得突兀，反而将整个房间格调拔高不少。【】大道至简，无论是中国、日本、泰国等东南亚地区，都可以寻到这种简朴侘寂风格的影子，这是我们东亚文化中共同的一部分。这位日本的杂志主编将150年的古宅改造成了温泉酒店，将日式简约发挥到了极致。（来源：lnstagram优选，由网易家居综合整理）\n",
      "https://ent.163.com/19/1209/09/EVUPOMRO00038FO9.html,心虚?李小璐晒旅游照又秒删 疑与PG One国外相会_网易娱乐,前段时间，李小璐和PGOne先后曝光了三段亲密视频引发热议。随后李小璐通过律师回应说视频是在离婚后录制，属于个人私生活，而且两人不可能在一起。但最近李小璐在社交平台上发布的照片似乎却在告诉大家两个人并没有结束恋爱关系。李小璐先是发布了三张在国外旅游的照片，但很快就删掉了动态，重新编辑后上传，现在动态里就剩下一张鲜花的照片。虽然李小璐秒删了动态，但还是有眼疾手快的网友把被删掉的两张照片保存了下来。其中一张照片是李小璐以车上的视角拍摄的，初看并没有什么奇特之处，可神通广大的网友却靠这张照片推断出了李小璐所在的位置。照片中半空中拉的红色横幅两边有螃蟹图案，这个图案是一个名为“Meadows”的美国学校举办活动时会用到的，活动地点在加州旧金山的米尔布雷。恰巧前几天PG One在微博上宣布他将在12月22日在美国加州洛杉矶开演唱会，这么看来他在国外还是有不少粉丝的。旧金山和洛杉矶虽然相隔500多公里，但都是在美国，而且米尔布雷距离旧金山国际机场开车仅需要一个多小时，交通方便。相同的时间点出现在如此之近的位置，两人疑似前后脚到达美国加州。而且李小璐在被网友发现后还心虚地删除了暴露位置的照片，引发不少网友猜测。\n",
      "https://home.163.com/19/1210/07/F012SP99001081EI.html,62岁老人自制创意鲁班家具 不用一颗钉子还可折叠_网易家居,前段时间，有个62岁的大爷制作出鲁班凳，在网上引起一片热议。本以为过了这么久，人们已经淡忘了这件事。没想到大爷还是闲不住，最近他的视频又在外网火了起来。不用电脑制图，不用锯子电钻就能做出这么些小玩意，难怪外国人都看呆了~阿木爷爷的视频是最淳朴的乡村风，做东西也是用最简单的工具，但是给人们带来的感觉，却是技艺上的震撼。最为出名的就是这些纯手工家具，他们之所以被称为鲁班凳、鲁班桌，就是因为没用一根钉子，一点缝合剂就将他们组合好了。更神奇的是他们还可以伸缩折叠！不需要的时候收起来也不占地方。打造这些家具都是大爷的基本操作，你也不用惊讶。再说说那些有趣的小玩意吧，光是看着就觉得复杂。像是那个抽水的装置，还有世博会的展览馆。这些装饰品要是上市了，恐怕会一扫而空吧。家居姐实在无法想象，光是展示出来的玩意儿就有这么多了，房子里面得有多少神奇的物件啊，真想大开一下眼界。老人和他的孙子一起住，几乎所有的玩具都是由他打造的。孙子很喜欢湖面上的船，老人就打造了一个木头的小船模型。不仅外观精致，还能顺水漂流。好像不管孩子喜欢什么，爷爷都能做得出来，这样的童年生活简直太幸福了~老人在门前种了许多花，天天做着自己喜欢的事，鲁班木制品虽然不是所有人的会，但是手工活还是相对简单的。你绝对想不到，一个著名的地产企业家也会做手工活吧，潘石屹就在节目中为张继科做过一副球拍，这种不谈钱的互动真是很神奇了。说实话，他做的家具比阿木爷爷的鲁班系列的还要好看一些呢，从这么特别的色彩和纹路上看，企业家也有一颗充满艺术的心啊。不看不知道，潘老师竟然这么有才华，（来源：Youtube阿木爷爷等，由网易家居综合整理）\n"
     ]
    }
   ],
   "source": [
    "!head 163news_d.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 继承新闻索引类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class NewsIndex(SearcherIIndexVII):\n",
    "    \"\"\"新闻内容索引，实现对CSV文件的索引、对长文的高亮摘要、对新闻链接的支持\n",
    "    \"\"\"\n",
    "    def __init__(self, docs_file): \n",
    "        \"\"\"初始化，用CSV文件中的title+content构建倒排索引，将url保存在link_list里\n",
    "        \n",
    "        Args:\n",
    "            docs_file:包含新闻URL、title和content的csv文件的文件名\n",
    "            \n",
    "        \"\"\"\n",
    "        self.index = dict() #标准倒排索引\n",
    "        self.index_b = dict() #ngram索引\n",
    "        self.max_id = 0\n",
    "        self.doc_list = [] \n",
    "        self.link_list = []\n",
    "        \n",
    "        with open(docs_file, 'r') as f:\n",
    "            f_csv = csv.reader(f)\n",
    "            for row in f_csv:\n",
    "                self.add_doc(row[1] + row[2])\n",
    "                self.link_list.append(row[0])\n",
    "\n",
    "    def highlighter(self, doc, query):\n",
    "        \"\"\"用query对doc进行HTML高亮\n",
    "        \n",
    "        Args:\n",
    "            doc:需要高亮的文档\n",
    "            word:要进行高亮的关键词(查询)\n",
    "            \n",
    "        Returns:\n",
    "            返回对关键词(查询)进行高亮的文档\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        #生成要进行高亮的关键词串集合\n",
    "        word_set = set()\n",
    "        query = query.lower()\n",
    "        query_parts = self.parse_query(query)\n",
    "        for query_part in query_parts:\n",
    "            if query_part[0] == 'e' or query_part[0] == 'f':\n",
    "                word_set.add(query_part[1:])\n",
    "                if len(query_part[1:]) > n:\n",
    "                    n = len(query_part[1:])\n",
    "            elif query_part[0] == 'c':\n",
    "                if len(query_part[1:]) > 1:\n",
    "                    for word in jieba.cut(query_part[1:]):\n",
    "                        word_set.add(word)\n",
    "                        if len(word) > n:\n",
    "                            n = len(word)\n",
    "        \n",
    "        #遍历文档替换高亮关键词串\n",
    "        doc_low = doc.lower()\n",
    "        i = 0\n",
    "        result = []\n",
    "        side_len = 10 #设置上下文保留的宽度\n",
    "        last_end = 0 #上一个命中关键词的末位位置\n",
    "        while True:\n",
    "            end_idx = i + n\n",
    "            if end_idx > len(doc_low):\n",
    "                end_idx = len(doc_low)\n",
    "            for j in range(end_idx, i, -1):\n",
    "                if doc_low[i:j] in word_set:\n",
    "                    break\n",
    "            if doc_low[i:j] in word_set:\n",
    "                #追加从上一个关键词末位到当前关键词开头的周围上下文\n",
    "                if i - last_end > 2 * side_len:\n",
    "                    result.append(doc[last_end:last_end+side_len])\n",
    "                    result.append('...')\n",
    "                    result.append(doc[i-side_len:i])\n",
    "                else:\n",
    "                    result.append(doc[last_end:i])\n",
    "                #追加高亮处理的关键词串\n",
    "                result.append(\n",
    "                    '<span style=\"color:red\">{}</span>'.format(doc[i:j]))\n",
    "                last_end = j\n",
    "#             else:\n",
    "#                 result.append(doc_low[i:j])\n",
    "            i = j\n",
    "            if i == len(doc_low):\n",
    "                right_idx = last_end + side_len\n",
    "                if right_idx > len(doc):\n",
    "                    result.append(doc[last_end:])\n",
    "                else:\n",
    "                    result.append(doc[last_end:right_idx])\n",
    "                    result.append('...')\n",
    "                break\n",
    "                \n",
    "        return ''.join(result)\n",
    "    \n",
    "    def search(self, query):\n",
    "        \"\"\"用query进行查询返回结果文档列表\n",
    "        \n",
    "        Args:\n",
    "            query:用户的(复合)布尔查询字符串\n",
    "            \n",
    "        Returns:\n",
    "            复合查询要求的(高亮)文档结果列表\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        query_new = self.conv_query(query)\n",
    "        for did in eval(query_new):\n",
    "            result.append([self.highlighter(self.doc_list[did], query), self.link_list[did]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_searcher = NewsIndex('163news_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://sports.163.com/19/1203/04/EVEPDFHV00058781.html\" target=\"_blank\"><span style=\"color:red\">梅西</span>获颁2019年金球奖...上分量最重的金球奖由<span style=\"color:red\">梅西</span>获得。阿根廷人力压范...球》的名义单独评选。<span style=\"color:red\">梅西</span>和C罗是金球奖有史以...锋格里兹曼排在第三，<span style=\"color:red\">梅西</span>无缘三甲。2019年对于<span style=\"color:red\">梅西</span>而言是悲喜交加的一年...球先生。德罗巴宣布了<span style=\"color:red\">梅西</span>的获奖，上届金球得主莫德里奇为<span style=\"color:red\">梅西</span>颁出金球奖杯。<span style=\"color:red\">梅西</span>在2009年、201...金靴两项大奖。金球：<span style=\"color:red\">梅西</span>银球：范戴克铜球：C...</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = '梅西'\n",
    "result = news_searcher.search(query)\n",
    "if result:\n",
    "    for doc,url in result:\n",
    "        display(HTML('<a href=\"{}\" target=\"_blank\">{}</a>'.format(url,doc)))\n",
    "else:\n",
    "    print('No result.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tomorrow多线程采集示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4.428255 seconds\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'http://google.com',\n",
    "    'http://facebook.com',\n",
    "    'http://youtube.com',\n",
    "    'http://baidu.com',\n",
    "    'http://yahoo.com',\n",
    "]\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from tomorrow import threads\n",
    "\n",
    "@threads(5)\n",
    "def download(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start = time.time()\n",
    "    responses = [download(url) for url in urls]\n",
    "    html = [response.text for response in responses]\n",
    "    end = time.time()\n",
    "    print(\"Time: %f seconds\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API采集示例 (参考 https://www.jianshu.com/p/c54e25349b77 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "态℃|不，你不想成为李子柒 http://3g.163.com/tech/19/1211/17/F04NR2FT000999D8.html\n",
      "\"腿骨折了想要跑步机”社会扶贫App被指奇葩需求多 http://3g.163.com/tech/19/1211/14/F04D2RA400097U82.html\n",
      "加拿大非法逮捕孟晚舟文件曝光 手机信息给了FBI http://3g.163.com/tech/19/1211/11/F045TTL400097U7S.html\n",
      "重返月球要花多少钱？一颗火箭可能要百亿 http://3g.163.com/tech/19/1211/11/F045M34P00097U81.html\n",
      "不是iPhone，库克谈苹果对人类最大贡献是啥 http://3g.163.com/tech/19/1211/10/F040UFT900097U7S.html\n",
      "德国电信运营商宣布将采用华为设备建设5G网络 http://3g.163.com/tech/19/1211/19/F050V2TE000999LD.html\n",
      "iOS 13.3发布：可以过滤iMessage垃圾信息了 http://3g.163.com/tech/19/1211/07/F03M5VFI00097U7T.html\n",
      "蔚来又在硅谷裁员141人 大砍自动驾驶团队 http://3g.163.com/tech/19/1211/08/F03QLD8V00097U7T.html\n",
      "VLOG25#：一起看荣耀V30 Pro镜头里的TeamLab有多美 http://3g.163.com/mobile/19/1211/17/F04NG6LM00119821.html\n",
      "英特尔公布技术路线图：10年后推1.4纳米工艺 http://3g.163.com/tech/19/1211/10/F03VTN4N00097U7T.html\n",
      "亚马逊向中国卖家开放新加坡站点 全球站点增至13个 http://3g.163.com/tech/19/1211/21/F055KTSN00097U7R.html\n",
      "苹果iOS更新：升级垃圾短信过滤能力 增家长控制功能 http://3g.163.com/tech/19/1211/20/F051PHDO00097U7S.html\n",
      "报名|5G·未来沙龙第二弹！这次聊应用、谈模式、讲落地 http://3g.163.com/tech/19/1211/14/F04E5F8900098IEO.html\n",
      "爆雷P2P让领导先跑？纪检部门通报首提\"特权挽损\" http://3g.163.com/tech/19/1211/12/F048K7AN00097U7R.html\n",
      "美参议员：苹果FB等应向执法人员提供加密用户数据 http://3g.163.com/tech/19/1211/07/F03N0FFJ00097U7R.html\n",
      "禁人脸识别和电子烟后 旧金山欲规管公共场所新技术测试 http://3g.163.com/tech/19/1211/20/F054RB3J00097U7T.html\n",
      "条形码联合发明人去世：这一符号几乎革新了每个行业 http://3g.163.com/tech/19/1211/20/F0531B6100097U81.html\n",
      "滴滴启动元旦春节安全保障工作 程维：安全弦不能松 http://3g.163.com/tech/19/1211/18/F04RGD0N00097U7R.html\n",
      "强者更强，科技繁荣让5座美国城市高薪工作越来越多 http://3g.163.com/tech/19/1211/14/F04DD8EA00097U7R.html\n",
      "华为出资5000万成立云计算公司 法定代表人为郑叶来 http://3g.163.com/tech/19/1211/20/F0526VKQ00097U7S.html\n",
      "贾跃亭个人破产重组风波再起 有债权人提议取消议案 http://3g.163.com/tech/19/1211/17/F04P9DUR00097U7R.html\n",
      "视觉中国:整改方向或砍掉违规业务 股票暂无停牌计划 http://3g.163.com/tech/19/1211/15/F04JNRJ600097U7R.html\n",
      "SpaceX宣布两周内发射第三批Starlink卫星 http://3g.163.com/tech/19/1211/07/F03MKJ2P00097U7S.html\n",
      "谷歌Facebook双双跌出美国最佳雇主年度榜单前十 http://3g.163.com/tech/19/1211/14/F04G30D400097U7R.html\n",
      "你有多少个工作微信群？经历过刷屏式\"收到\"吗 http://3g.163.com/tech/19/1211/14/F04ED5FE00097U7R.html\n",
      "央行科技司司长：抓紧研究区块链等17项行业标准 http://3g.163.com/tech/19/1211/12/F047P24I000998GP.html\n",
      "条形码共同发明人去世享年94岁：他改变了很多行业 http://3g.163.com/tech/19/1211/10/F0407MF500097U7T.html\n",
      "视觉中国开盘跌停 公司网站自查整改暂停服务 http://3g.163.com/tech/19/1211/09/F03U39U000097U7R.html\n",
      "Redmi K30发布后，小米集团股价大涨近7% http://3g.163.com/tech/19/1211/13/F04C7J8B000999LD.html\n",
      "中国联通用户如何在iPhone上体验VoLTE功能？ http://3g.163.com/tech/19/1211/12/F048EHK7000999LD.html\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,30,10):\n",
    "    url = 'https://3g.163.com/touch/reconstruct/article/list/BA8D4A3Rwangning/{}-10.html'.format(i)\n",
    "    r = requests.get(url)\n",
    "    results = json.loads(r.text[9:-1])\n",
    "    for result in results['BA8D4A3Rwangning']:\n",
    "        print(result['title'], result['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有些页面用默认设置无法采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://book.douban.com/tag/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 采集有反爬虫机制的网页\n",
    "浏览器正常访问 -> Copy as Curl -> 粘贴至 https://curl.trillworks.com/ 转成Requests请求代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = {\n",
    "    \n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Referer': 'https://book.douban.com/',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "}\n",
    "\n",
    "response = requests.get('https://book.douban.com/tag/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C', headers=headers, cookies=cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神经网络与深度学习\n",
      "Python深度学习\n",
      "Python神经网络编程\n",
      "深度学习入门\n",
      "深度学习的数学\n",
      "Neural Networks and Deep Learning\n",
      "神经网络与机器学习（原书第3版）\n",
      "神经网络与机器学习\n",
      "连接组：造就独一无二的你\n",
      "深入理解神经网络\n",
      "意识的宇宙\n",
      "人工智能 （第2版）\n",
      "神经网络在应用科学和工程中的应用\n",
      "Neural Networks and Learning Machines\n",
      "Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
      "Neural Network Methods in Natural Language Processing\n",
      "神经网络设计\n",
      "Make Your Own Neural Network\n",
      "图解深度学习与神经网络：从张量到TensorFlow实现\n",
      "深度学习\n"
     ]
    }
   ],
   "source": [
    "sel = etree.HTML(response.text) #解析\n",
    "nodes = sel.xpath(\"//li[@class='subject-item']\")\n",
    "for node in nodes:\n",
    "    title = node.xpath('.//h2/a/text()')[0].strip()\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
